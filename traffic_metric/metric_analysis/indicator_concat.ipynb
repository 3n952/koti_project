{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분할 - 평일 / 주말 >> 보고서용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "near_link concat start\n",
      "near_link concat completed\n",
      "link_std_spd concat start\n",
      "link_std_spd concat completed\n",
      "save to csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_dir = r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation'\n",
    "\n",
    "link_std_spd_columns = ['vLinkId', 'total_traffic', 'avg_spd', 'timecode', 'weekendcode', 'std_spd']\n",
    "near_link_spd_count_columns = ['vLinkId', '5km_spd_count', '10km_spd_count', '15km_spd_count', '20km_spd_count', '25km_spd_count', '30km_spd_count'\n",
    "                        ,'35km_spd_count', '40km_spd_count', '45km_spd_count', '50km_spd_count', 'ovr50km_spd_count',\n",
    "                        'total_traffic','timecode', 'weekendcode']\n",
    "near_link_spd_columns = ['vLinkID', 'std_spd', 'timecode', 'weekendcode']\n",
    "\n",
    "is_first1 = True\n",
    "is_first2 = True\n",
    "is_first3 = True\n",
    "\n",
    "mode = input('choose mode')\n",
    "\n",
    "print('near_link concat start')\n",
    "for i in range(9, 18):\n",
    "    input_files = mode+f'_near_link_spd_{i}.txt'\n",
    "    file_path = os.path.join(input_dir, input_files)\n",
    "    input_df1 = pd.read_csv(file_path, header = None)\n",
    "    input_df1.columns = near_link_spd_columns\n",
    "    \n",
    "    if is_first1:\n",
    "        concat_df1 = input_df1\n",
    "        is_first1 = False\n",
    "    else:\n",
    "        concat_df1 = pd.concat([concat_df1, input_df1])\n",
    "print('near_link concat completed')\n",
    "\n",
    "print('link_std_spd concat start')\n",
    "for i in range(9, 18):\n",
    "    input_files = mode+f'_link_std_spd_{i}.txt'\n",
    "    file_path = os.path.join(input_dir, input_files)\n",
    "    input_df2= pd.read_csv(file_path, header = None)\n",
    "    input_df2.columns = link_std_spd_columns\n",
    "    \n",
    "    if is_first2:\n",
    "        concat_df2 = input_df2\n",
    "        is_first2 = False\n",
    "    else:\n",
    "        concat_df2 = pd.concat([concat_df2, input_df2])\n",
    "print('link_std_spd concat completed')\n",
    "\n",
    "print('save to csv')\n",
    "concat_df2 = concat_df2[['vLinkId', 'weekendcode', 'timecode', 'avg_spd', 'std_spd']]\n",
    "concat_df1 = concat_df1[['vLinkId', 'weekendcode', 'timecode', '5km_spd_count', '10km_spd_count', '15km_spd_count'\n",
    "                         , '20km_spd_count', '25km_spd_count', '30km_spd_count','35km_spd_count', '40km_spd_count'\n",
    "                         , '45km_spd_count', '50km_spd_count', 'ovr50km_spd_count', 'total_traffic']]\n",
    "\n",
    "concat_df1.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation'+'\\\\'+mode+'_near_link_spd_20231211_20231217.csv', index = False)\n",
    "concat_df2.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation'+'\\\\'+mode+'_link_std_spd_20231211_20231217.csv', index = False)\n",
    "\n",
    "print('near_link 평일/주말 분할 start')\n",
    "link_std_work = concat_df2[concat_df2['weekendcode'] == 0]\n",
    "link_std_weekend = concat_df2[concat_df2['weekendcode'] == 1]\n",
    "link_std_work.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\metric\\workdays'+'\\\\link_std_spd_20231211_20231217.csv', index = False)\n",
    "link_std_weekend.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\metric\\weekends'+'\\\\link_std_spd_20231211_20231217.csv', index = False)\n",
    "print('near_link 평일/주말 분할 completed')\n",
    "\n",
    "print('near_link 평일/주말 분할 start')\n",
    "near_link_work = concat_df1[concat_df1['weekendcode'] == 0]\n",
    "near_link_weekend = concat_df1[concat_df1['weekendcode'] == 1]\n",
    "near_link_work.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\metric\\workdays'+'\\\\near_link_spd_20231211_20231217.csv', index = False)\n",
    "near_link_weekend.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\metric\\weekends'+'\\\\near_link_spd_20231211_20231217.csv', index = False)\n",
    "print('near_link 평일/주말 분할 completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(concat_df1['weekendcode'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "near_link 평일/주말 분할 start\n",
      "near_link 평일/주말 분할 completed\n",
      "near_link 평일/주말 분할 start\n",
      "near_link 평일/주말 분할 completed\n"
     ]
    }
   ],
   "source": [
    "print('near_link 평일/주말 분할 start')\n",
    "link_std_work = concat_df2[concat_df2['weekendcode'] == 0]\n",
    "link_std_weekend = concat_df2[concat_df2['weekendcode'] == 1]\n",
    "link_std_work.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation\\link_std_work_20231211_20231217.csv', index = False)\n",
    "link_std_weekend.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation\\link_std_weekend_20231211_20231217d.csv', index = False)\n",
    "print('near_link 평일/주말 분할 completed')\n",
    "\n",
    "print('near_link 평일/주말 분할 start')\n",
    "near_link_work = concat_df1[concat_df1['weekendcode'] == 0]\n",
    "near_link_weekend = concat_df1[concat_df1['weekendcode'] == 1]\n",
    "near_link_work.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation\\near_link_work_20231211_20231217w.csv', index = False)\n",
    "near_link_weekend.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation\\near_link_weekend_20231211_20231217d.csv', index = False)\n",
    "print('near_link 평일/주말 분할 completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRUCK, BUS의 지표를 하나로 합치기 >> 보고서 + 내부검증용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# bus 용\n",
    "# near_link_spd csv \n",
    "bus_near_df = pd.read_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation\\MOCT_NEW\\BUS_near_link_spd.csv')\n",
    "# link_std_spd csv\n",
    "bus_link_df = pd.read_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation\\MOCT_NEW\\BUS_link_std_spd.csv')\n",
    "\n",
    "# truck 용\n",
    "# near_link_spd csv \n",
    "truck_near_df = pd.read_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation\\MOCT_NEW\\TRUCK_near_link_spd.csv')\n",
    "# link_std_spd csv\n",
    "truck_link_df = pd.read_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\validation\\MOCT_NEW\\TRUCK_link_std_spd.csv')\n",
    "\n",
    "# near_link_spd concat\n",
    "near_concat = pd.concat([bus_near_df, truck_near_df])\n",
    "\n",
    "# link_std_spd concat\n",
    "link_concat = pd.concat([bus_link_df, truck_link_df])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# link_std_spd / near_link_spd 지표 concat해서 하나의 csv파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "is_first = [True] * 2\n",
    "sample_dir = r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\metric_samples'\n",
    "mode = 'BUS'\n",
    "\n",
    "# near_link_spd 지표 concat\n",
    "for i in range(9, 18):\n",
    "    input_files = mode+f'_near_link_spd_{i}.txt'\n",
    "    input_dir = os.path.join(sample_dir, input_files)\n",
    "    input_df = pd.read_csv(input_dir, header = None)\n",
    "    input_df.columns = ['vLinkId', 'timecode', 'weekendcode', 'near_spd_diff']\n",
    "    \n",
    "    if is_first[0]:\n",
    "        near_concat_df = input_df\n",
    "        is_first[0] = False\n",
    "    else:\n",
    "        near_concat_df = pd.concat([near_concat_df, input_df])\n",
    "\n",
    "# 기존의 다른 열을 유지하면서 'timecode' 열만 변경\n",
    "#near_concat_df.loc[:, 'timecode'] = np.ceil(near_concat_df['timecode']).astype(int) + 1\n",
    "# CSV 파일로 저장\n",
    "near_concat_df.to_csv(rf\"C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\metrics_csv\\{mode}_near_link_spd.csv\", index=False)\n",
    "\n",
    "\n",
    "# link_std_spd 지표 concat\n",
    "for i in range(9, 18):\n",
    "    input_files = mode+f'_prev_link_std_spd_{i}.txt' # 우진과장님 산출 방식 차용\n",
    "    input_dir = os.path.join(sample_dir, input_files)\n",
    "    input_df = pd.read_csv(input_dir, header = None)\n",
    "    input_df.columns = ['vLinkId', 'total_counts', 'avg_spd', 'timecode', 'weekendcode', 'std_spd']\n",
    "    \n",
    "    if is_first[1]:\n",
    "        link_concat_df = input_df\n",
    "        is_first[1] = False\n",
    "    else:\n",
    "        link_concat_df = pd.concat([link_concat_df, input_df])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "link_concat_df.to_csv(rf\"C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\metrics_csv\\{mode}_link_std_spd.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크와 지표를 링크 단위로 concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\QBIC\\AppData\\Local\\Temp\\ipykernel_4668\\4265499908.py:11: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(rf'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\metrics_csv\\{mode}_{indicator}.csv', header = None)\n",
      "C:\\Users\\QBIC\\AppData\\Local\\Temp\\ipykernel_4668\\4265499908.py:33: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  merged_gdf.to_file(rf\"C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\metric_with_network\\{timerange}\\{mode}_{indicator}.shp\")\n",
      "c:\\Users\\QBIC\\miniconda3\\envs\\data\\lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'weekendcode' to 'weekendcod'\n",
      "  ogr_write(\n",
      "c:\\Users\\QBIC\\miniconda3\\envs\\data\\lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: 2GB file size limit reached for C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\metric_with_network\\20231211_20231217\\BUS_link_std_spd.dbf. Going on, but might cause compatibility issues with third party software\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "# 모든 데이터 프레임을 병합하는 함수 정의\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "mode = 'BUS'\n",
    "indicator = 'link_std_spd'\n",
    "timerange = '20231211_20231217'\n",
    "\n",
    "\n",
    "network_df = gpd.read_file(r\"C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\moct_network\\MOCT_LINK_20240325.shp\")\n",
    "df = pd.read_csv(rf'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\metrics_csv\\{mode}_{indicator}.csv', header = None)\n",
    "\n",
    "if indicator == 'near_link_spd':\n",
    "    df.columns = ['link_id', 'timecode', 'weekendcode', 'clsd']\n",
    "else:\n",
    "    assert indicator == 'link_std_spd'\n",
    "    df.columns = ['link_id', 'total_count', 'avg_spd', 'timecode','weekendcode', 'ssd']\n",
    "    df = df.drop(['avg_spd', 'total_count'], axis=1)\n",
    "\n",
    "network_df[\"link_id\"] = network_df[\"link_id\"].astype(str)\n",
    "df[\"link_id\"] = df[\"link_id\"].astype(str)\n",
    "\n",
    "def merge_dfs(network_df, df):\n",
    "    return pd.merge(network_df, df, on=['link_id'], how='inner')\n",
    "\n",
    "merged_gdf = merge_dfs(network_df, df)\n",
    "merged_gdf.set_crs(epsg=5179, inplace=True)\n",
    "\n",
    "# 속도 데이터가 포함된 새로운 SHP 파일 저장\n",
    "# 그 중 sample\n",
    "# merged_gdf = merged_gdf.sample(frac=0.5, random_state=42)\n",
    "\n",
    "merged_gdf.to_file(rf\"C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\metric_with_network\\{timerange}\\{mode}_{indicator}.shp\")\n",
    "\n",
    "# gpd to pickle\n",
    "#merged_gdf.to_pickle(rf\"C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\metric_with_network\\{timerange}\\{mode}_{indicator}.plk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cencus 단위 지표 통합 -> 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\QBIC\\miniconda3\\envs\\data\\lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: 2GB file size limit reached for data_valid.dbf. Going on, but might cause compatibility issues with third party software\n",
      "  ogr_write(\n",
      "c:\\Users\\QBIC\\miniconda3\\envs\\data\\lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: 2GB file size limit reached for data_valid.shp. Going on, but might cause compatibility issues with third party software\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "merged_gdf = merged_gdf.sample(frac=0.5, random_state=42)\n",
    "merged_gdf.to_file(\"data_valid.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11대 위험 운전 지표 합치기 >> 내부 검증용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# 모든 데이터 프레임을 병합하는 함수 정의\n",
    "def merge_dfs(df_left, df_right):\n",
    "    return pd.merge(df_left, df_right, on=['vLinkID', 'total_traffic'], how='outer')\n",
    "\n",
    "input_dir = r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\raw_metric'\n",
    "\n",
    "indicate_dict = {\n",
    "'ovrspd': ['vLinkID', 'ovrspd', 'total_traffic'],\n",
    "'ltrm_ovrspd': ['vLinkID', 'ltrm_ovrspd', 'total_traffic'],\n",
    "'rapid_acc': ['vLinkID', 'rapid_acc', 'total_traffic'],\n",
    "'rapid_start' : ['vLinkID', 'rapid_start', 'total_traffic'],\n",
    "'rapid_decel' : ['vLinkID', 'rapid_decel', 'total_traffic'],\n",
    "'rapid_stop' : ['vLinkID', 'rapid_stop', 'total_traffic'],\n",
    "'rapid_lane_change' : ['vLinkID', 'rapid_lane_change', 'total_traffic'],\n",
    "'rapid_overtake' : ['vLinkID', 'rapid_overtake', 'total_traffic'],\n",
    "'rapid_turn' : ['vLinkID', 'rapid_turn', 'total_traffic'],\n",
    "'rapid_Uturn' : ['vLinkID', 'rapid_Uturn', 'total_traffic']\n",
    "}\n",
    "\n",
    "indicator_list = ['ovrspd', 'ltrm_ovrspd', 'rapid_acc', 'rapid_start', 'rapid_decel', 'rapid_stop',\n",
    "                  'rapid_lane_change', 'rapid_overtake', 'rapid_turn', 'rapid_Uturn']\n",
    "\n",
    "total_indicate = []\n",
    "\n",
    "print('11대 위험운전 지표 산출을 위한 dataFrame merge start')\n",
    "for i in range(len(indicator_list)):\n",
    "    input_files = f'BUS_{indicator_list[i]}.txt'\n",
    "    file_path = os.path.join(input_dir, input_files)\n",
    "    input_df = pd.read_csv(file_path, header = None)\n",
    "    input_df.columns = indicate_dict[indicator_list[i]]\n",
    "    # 한 지표에 대한 df 자체를 리스트에 담기\n",
    "    total_indicate.append(input_df)\n",
    "\n",
    "total_11th_indicate = reduce(lambda left, right: pd.merge(left, right, on=['vLinkID', 'total_traffic'], how='outer'), total_indicate)\n",
    "\n",
    "# 나머지 열의 합산을 새로운 열 'summed_metrics'에 저장\n",
    "total_11th_indicate['11th_metrics'] = total_11th_indicate.drop(['vLinkID', 'total_traffic'], axis=1).sum(axis=1)\n",
    "\n",
    "# reduce 함수를 사용하여 모든 데이터 프레임 병합\n",
    "total_11th_indicate = total_11th_indicate.drop(['ovrspd', 'ltrm_ovrspd', 'rapid_acc', 'rapid_start', 'rapid_decel', \n",
    "                                                'rapid_stop', 'rapid_lane_change', 'rapid_overtake', 'rapid_turn', 'rapid_Uturn'], axis=1)\n",
    "\n",
    "print('save to csv')\n",
    "total_11th_indicate = total_11th_indicate[['vLinkId', '11th_metrics', 'total_traffic']]\n",
    "total_11th_indicate.to_csv(r'C:\\Users\\QBIC\\Desktop\\workspace\\koti_project\\dataset\\metric\\eleven_metric_20231211_20231217.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOCT 네트워크와 결합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\QBIC\\miniconda3\\envs\\data\\lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'lanes' to 'lanes_1'\n",
      "  ogr_write(\n",
      "c:\\Users\\QBIC\\miniconda3\\envs\\data\\lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: 2GB file size limit reached for link_std_spd.dbf. Going on, but might cause compatibility issues with third party software\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 속도 데이터가 추가된 SHP 파일이 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "moct_shp = gpd.read_file(r\"F:\\Unsung\\[2024-11-29]NODELINKDATA\\MOCT_LINK\\MOCT_LINK.shp\")  # 기존 도로 데이터\n",
    "link_std_spd = pd.read_csv(r'C:\\Users\\QBIC\\Desktop\\GANSUN_SAFE_INDEX_2023_NEW_20250219.csv') # 링크와 병합할 데이터 -> 지표들\n",
    "\n",
    "\n",
    "link_std_spd = link_std_spd.rename(columns={\"its_id\": \"LINK_ID\"}) # 참조할 column 통일\n",
    "\n",
    "moct_shp[\"LINK_ID\"] = moct_shp[\"LINK_ID\"].astype(str)\n",
    "link_std_spd[\"LINK_ID\"] = link_std_spd[\"LINK_ID\"].astype(str)\n",
    "\n",
    "link_std_spd.drop(columns=['road_name', 'road_rank', 'length', 'emd_id', 'fdvr', 'cr', 'clsd', 'rde', 'lnpj', 'an', 'dder', 'yr', 'p2pj', 'hvr', 'epdo', 'utsi'], inplace=True)\n",
    "\n",
    "# 3️⃣ link_id 기준으로 속도 데이터와 기존 SHP 파일 병합\n",
    "merged_gdf = moct_shp.merge(link_std_spd, on=\"LINK_ID\", how=\"left\")\n",
    "\n",
    "# 4️⃣ 속도 데이터가 포함된 새로운 SHP 파일 저장\n",
    "merged_gdf.to_file(\"link_std_spd.shp\", driver=\"ESRI Shapefile\")\n",
    "\n",
    "print(\"✅ 속도 데이터가 추가된 SHP 파일이 저장되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
